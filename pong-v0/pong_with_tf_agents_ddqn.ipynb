{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Atari with tf_agents.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25JzmqrOU_B8",
        "colab_type": "text"
      },
      "source": [
        "# Install dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibDdIJu70-aI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  !sudo apt-get install -y xvfb ffmpeg\n",
        "  !pip install -q 'gym==0.10.11'\n",
        "  !pip install -q 'imageio==2.4.0'\n",
        "  !pip install -q PILLOW\n",
        "  !pip install -q 'pyglet==1.3.2'\n",
        "  !pip install -q pyvirtualdisplay\n",
        "  !pip install -q --upgrade tensorflow-probability\n",
        "  !pip install -q tf-agents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtglCR_1VWjO",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zn4GkIp2f6A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import base64\n",
        "import os\n",
        "import shutil\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.environments import suite_atari, suite_gym, tf_py_environment, batched_py_environment, parallel_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.policies import random_py_policy, policy_saver, random_tf_policy\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.utils import common\n",
        "\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.trajectories import time_step as ts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RTy79HR4F--",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display = pyvirtualdisplay.Display(visible=0, size=(1400,900)).start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkcSehsBbQsY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-K1wSVSabQrZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = '/gdrive/My Drive/Atari_DQN_Data/checkpoint/'\n",
        "policy_dir = '/gdrive/My Drive/Atari_DQN_Data/policy/'\n",
        "\n",
        "if not os.path.exists(checkpoint_dir):\n",
        "    os.makedirs(checkpoint_dir)\n",
        "\n",
        "if not os.path.exists(policy_dir):\n",
        "    os.makedirs(policy_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZU9RicqDjjK",
        "colab_type": "text"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDvUOjao6UMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_iterations = 250000\n",
        "\n",
        "initial_collect_steps = 200\n",
        "collect_steps_per_iteration = 10\n",
        "replay_buffer_max_length = 100000\n",
        "\n",
        "batch_size = 32\n",
        "learning_rate = 2.5e-3\n",
        "log_interval = 5000\n",
        "\n",
        "num_eval_episodes = 10\n",
        "eval_interval = 25000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aC0nWQsT7E4d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env_name = 'Pong-v0'\n",
        "\n",
        "ATARI_FRAME_SKIP = 4\n",
        "\n",
        "max_episode_frames = 108000\n",
        "\n",
        "env = suite_atari.load(\n",
        "    env_name,\n",
        "    max_episode_steps = max_episode_frames / ATARI_FRAME_SKIP,\n",
        "    gym_env_wrappers = suite_atari.DEFAULT_ATARI_GYM_WRAPPERS_WITH_STACKING\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEGmzO0GDr8u",
        "colab_type": "text"
      },
      "source": [
        "# Take a peek at the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qXYMcpg_8j_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "time_step = env.reset()\n",
        "PIL.Image.fromarray(env.render())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCnsG8I-Dyxh",
        "colab_type": "text"
      },
      "source": [
        "# Create environments for training and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShWvlk5-AwXU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_py_env = suite_atari.load(\n",
        "    env_name,\n",
        "    max_episode_steps = max_episode_frames / ATARI_FRAME_SKIP,\n",
        "    gym_env_wrappers = suite_atari.DEFAULT_ATARI_GYM_WRAPPERS_WITH_STACKING\n",
        ")\n",
        "\n",
        "eval_py_env = suite_atari.load(\n",
        "    env_name,\n",
        "    max_episode_steps = max_episode_frames / ATARI_FRAME_SKIP,\n",
        "    gym_env_wrappers = suite_atari.DEFAULT_ATARI_GYM_WRAPPERS_WITH_STACKING\n",
        ")\n",
        "\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3Y5A5a6D9Ym",
        "colab_type": "text"
      },
      "source": [
        "# Create Neural Network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SV7AAJ1VDKpE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AtariQNetwork(q_network.QNetwork):\n",
        "    def call(self, observation, step_type=None, network_state=(), training=False):\n",
        "        observation = tf.cast(observation, tf.float32)\n",
        "        observation = observation / 255\n",
        "\n",
        "        return super(AtariQNetwork, self).call(observation, step_type=step_type, network_state=network_state, training=training)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sx5wS-2XFxDc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fc_layer_params = (512,)\n",
        "conv_layer_params = ((32, (8,8), 4), (64, (4,4),2), (64, (3,3), 1))\n",
        "\n",
        "q_net = AtariQNetwork(\n",
        "    input_tensor_spec = train_env.observation_spec(),\n",
        "    action_spec = train_env.action_spec(),\n",
        "    conv_layer_params=conv_layer_params,\n",
        "    fc_layer_params=fc_layer_params\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-LzYwHdEO5B",
        "colab_type": "text"
      },
      "source": [
        "# Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2niereOG97U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.compat.v1.train.RMSPropOptimizer(\n",
        "    learning_rate=learning_rate,\n",
        "    decay = 0.95,\n",
        "    momentum=0.0,\n",
        "    epsilon=0.00001,\n",
        "    centered = True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sbe32io5EgWM",
        "colab_type": "text"
      },
      "source": [
        "# Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8LjG0NeLVZ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "observation_spec = train_env.observation_spec()\n",
        "time_step_spec = train_env.time_step_spec()\n",
        "\n",
        "action_spec = train_env.action_spec()\n",
        "\n",
        "target_update_period = 2000\n",
        "\n",
        "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    time_step_spec = time_step_spec,\n",
        "    action_spec = action_spec,\n",
        "    q_network = q_net,\n",
        "    optimizer = optimizer,\n",
        "    epsilon_greedy = 0.01,\n",
        "    n_step_update = 1.0,\n",
        "    target_update_tau = 1.0,\n",
        "    target_update_period = target_update_period,\n",
        "    td_errors_loss_fn = common.element_wise_huber_loss,\n",
        "    gamma = 0.99,\n",
        "    reward_scale_factor = 1.0,\n",
        "    gradient_clipping = None,\n",
        "    debug_summaries = False,\n",
        "    summarize_grads_and_vars = False,\n",
        "    train_step_counter = global_step\n",
        ")\n",
        "\n",
        "agent.initialize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wpNA5OQEjVV",
        "colab_type": "text"
      },
      "source": [
        "# Replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsekZF4-YgWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec = agent.collect_data_spec,\n",
        "    batch_size = train_env.batch_size,\n",
        "    max_length = replay_buffer_max_length\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sj80l5ksOgpY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
        "    env = train_env,\n",
        "    policy = agent.collect_policy,\n",
        "    observers=[replay_buffer.add_batch],\n",
        "    num_steps = collect_steps_per_iteration\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1CjZUvPAtVY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random_policy = random_tf_policy.RandomTFPolicy(\n",
        "    time_step_spec = train_env.time_step_spec(),\n",
        "    action_spec = train_env.action_spec()\n",
        ")\n",
        "\n",
        "\n",
        "initial_collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
        "    env = train_env,\n",
        "    policy = random_policy,\n",
        "    observers=[replay_buffer.add_batch],\n",
        "    num_steps = initial_collect_steps\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lli0bUfLR95-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "initial_collect_driver.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Vad9SukEsOa",
        "colab_type": "text"
      },
      "source": [
        "# Checkpointer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtYVeCbc4Zdx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_checkpointer = common.Checkpointer(\n",
        "    ckpt_dir=checkpoint_dir,\n",
        "    max_to_keep=1,\n",
        "    agent=agent,\n",
        "    policy=agent.policy,\n",
        "    replay_buffer=replay_buffer,\n",
        "    global_step=global_step\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUCxsLzWEwU7",
        "colab_type": "text"
      },
      "source": [
        "# Policy saver"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FL8mlphW8Yik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf_policy_saver = policy_saver.PolicySaver(agent.policy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqMVPxiHEzpf",
        "colab_type": "text"
      },
      "source": [
        "# Some metric for evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezuoqVAKAMeW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAcjiLD_SDsc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls = 3,\n",
        "    sample_batch_size = batch_size,\n",
        "    num_steps=2\n",
        ").prefetch(3)\n",
        "\n",
        "iterator = iter(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbBR0arTFAmg",
        "colab_type": "text"
      },
      "source": [
        "# Agent training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKDJ52ZEUbC7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agent.train = common.function(agent.train)\n",
        "\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "returns = [avg_return]\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "    collect_driver.run()\n",
        "\n",
        "    experience, unused_info = next(iterator)\n",
        "    train_loss = agent.train(experience).loss\n",
        "\n",
        "    step = agent.train_step_counter.numpy()\n",
        "\n",
        "    if step % log_interval == 0:\n",
        "        print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "\n",
        "    if step % eval_interval == 0:\n",
        "        avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "        returns.append(avg_return)\n",
        "\n",
        "        #global_step = tf.compat.v1.train.get_global_step()\n",
        "\n",
        "        #train_checkpointer.save(global_step)\n",
        "\n",
        "# Save the policy at the end of training so that it can be easily deployed.\n",
        "#tf_policy_saver.save(policy_dir)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9SUrBJ2fT1f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "global_step = tf.compat.v1.train.get_global_step()\n",
        "train_checkpointer.save(global_step)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7pHQFj4fesN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf_policy_saver.save(policy_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnFq3fovkUsV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iterations = range(0, num_iterations + 1, eval_interval)\n",
        "plt.plot(iterations, returns)\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylim(top=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fdrv77vNtrua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def embed_mp4(filename):\n",
        "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
        "  video = open(filename,'rb').read()\n",
        "  b64 = base64.b64encode(video)\n",
        "  tag = '''\n",
        "  <video width=\"640\" height=\"480\" controls>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "  </video>'''.format(b64.decode())\n",
        "\n",
        "  return IPython.display.HTML(tag)\n",
        "\n",
        "def create_policy_eval_video(policy, filename, num_episodes=5, fps=30):\n",
        "  filename = filename + \".mp4\"\n",
        "  with imageio.get_writer(filename, fps=fps) as video:\n",
        "    for _ in range(num_episodes):\n",
        "      time_step = eval_env.reset()\n",
        "      video.append_data(eval_py_env.render())\n",
        "      while not time_step.is_last():\n",
        "        action_step = policy.action(time_step)\n",
        "        time_step = eval_env.step(action_step.action)\n",
        "        video.append_data(eval_py_env.render())\n",
        "  return embed_mp4(filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c4PeQwgvjZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "create_policy_eval_video(agent.policy, \"trained-agent\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcYjKFvmvm1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "create_policy_eval_video(random_policy, \"random-agent\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}